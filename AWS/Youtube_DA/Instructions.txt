Data Source:

Kaggle - https://www.kaggle.com/datasets/datasnaek/youtube-new

Prerequisites:

- Download Dataset from Kaggle
- AWS CLI installed
	Configure AWS Account
		++ aws configure
		++ <<Access Key ID>>
		++ <<Secret Access Key>>
		++ <<Region>>
		++ <<Format>>
		++ aws s3 ls

Step-by-step:

1) Creating S3 Bucket
	Template name: s3://company-raw-awsregion-awsaccountID-env/source/source_region/tablename/year=yyyy/month=mm/day=dd/table_<yearmonthday>.<file_format>

	env = dev, test, prod
	source = name or indicator of source
	source_region = region of data source

2) Copy the data to S3, using AWS CLI
	Navigate to the data folder
		++ cd <<data_folder_path>>
	Copying data to S3 bucket
		# To copy all JSON Reference data to same location:
		++ aws s3 cp . s3://fh-youtube-raw-useast1-dev/youtube/raw_statistics_reference_data/ --recursive --exclude "*" --include "*.json"

		# To copy all data files to its own location, following Hive-style patterns:
		++ aws s3 cp CAvideos.csv s3://fh-youtube-raw-useast1-dev/youtube/raw_statistics/region=ca/
		++ aws s3 cp DEvideos.csv s3://fh-youtube-raw-useast1-dev/youtube/raw_statistics/region=de/
		++ aws s3 cp FRvideos.csv s3://fh-youtube-raw-useast1-dev/youtube/raw_statistics/region=fr/
		++ aws s3 cp GBvideos.csv s3://fh-youtube-raw-useast1-dev/youtube/raw_statistics/region=gb/
		++ aws s3 cp INvideos.csv s3://fh-youtube-raw-useast1-dev/youtube/raw_statistics/region=in/
		++ aws s3 cp JPvideos.csv s3://fh-youtube-raw-useast1-dev/youtube/raw_statistics/region=jp/
		++ aws s3 cp KRvideos.csv s3://fh-youtube-raw-useast1-dev/youtube/raw_statistics/region=kr/
		++ aws s3 cp MXvideos.csv s3://fh-youtube-raw-useast1-dev/youtube/raw_statistics/region=mx/
		++ aws s3 cp RUvideos.csv s3://fh-youtube-raw-useast1-dev/youtube/raw_statistics/region=ru/
		++ aws s3 cp USvideos.csv s3://fh-youtube-raw-useast1-dev/youtube/raw_statistics/region=us/

3) Use AWS Glue and create a crawler in order to create a Data Catalog for the initial data

	Note: You have to create a role to establish connections between S3 and Glue. Also you have to create a Database to store the table generated by the crawler

4) Run the crawler
	The crawler will start identifying the schema of the Initial Data. However, because of the structure of the json file, the crawler generate a table with 3 columns ('kind','tag', and items') grouping all the items (target data) in just one column as a array but we actually need only the data inside this array. 

	To solve this is required to pre-process, clean the initial data and transform it into a parquet file.

5) Using AWS Lambda to pre-process the initial data
	Create lambda function and a role to connect to S3 buckets

6) Introduce the ETL python code accomplish the pre-processing needed
	Add the necessary layers to run the lambda function successfully
	Run a test
	Execute the function

7) Running crawlers
	Run the crawler jobs to create the data catalog table for the JSON and CSV files.

8) Checking Data Catalog Table Schemas
	If it is required to change the data type of a column, you can do it directly on the table but is necessary to delete the file and running the lambda function again. Because the lambda function use "Append" method, it will reuse the existing schema.

9) Moving all data into a reporting bucket
	Create a Glue ETL job
	Assign Partition keys
	Create Data Catalog Database
	Assign IAM Role
	Run the Job

10) Use AWS QuickSight to create Dashboards using the created Data Catalog table from the reporting bucket (Athena)


		
	





		


